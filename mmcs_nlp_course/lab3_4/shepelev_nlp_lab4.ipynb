{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc7bd67f-3e1c-4cad-9ac9-bee85f0f40d4",
   "metadata": {},
   "source": [
    "Шепелев Д."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd48e8b-6d4f-4f9b-8d85-1c36453c5ea7",
   "metadata": {},
   "source": [
    "# **Моделирование языка на уровне символов**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c888ef-e584-4450-bc8e-8e6ca038aee0",
   "metadata": {},
   "source": [
    "**В качестве входных данных я выбрал текст книги \"451 градус по фаренгейту\", который сохранил в .txt-формате**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba7ff87-03d8-463a-81a6-3bf41967a075",
   "metadata": {},
   "source": [
    "## **Импорты**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2b522fa-b810-4d33-9e80-607c369a2c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8487d435-fc17-44f3-b773-6f8ff4f62a9f",
   "metadata": {},
   "source": [
    "## **Чтение и обработка текста**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "41c83b0a-0f21-41fa-bc40-7ace9432ab30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Общая длина: 270915\n",
      "Уникальные символы: 88\n"
     ]
    }
   ],
   "source": [
    "with open('451_full.txt', 'r', encoding=\"utf8\") as fp:\n",
    "    text = fp.read()\n",
    "    \n",
    "start_index = text.find('Если тебе дадут линованную бумагу, пиши поперек.')\n",
    "end_index = text.find('~~~ Конец ~~~')\n",
    "\n",
    "text = text[start_index:end_index]\n",
    "char_set = set(text)\n",
    "\n",
    "print(f'Общая длина: {len(text)}')\n",
    "print(f'Уникальные символы: {len(char_set)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38207de8-d5b3-403e-95a1-9d3872a68c1a",
   "metadata": {},
   "source": [
    "### **Кодирование текста**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c862d57d-d976-467c-b588-7f4defe74c17",
   "metadata": {},
   "source": [
    "Определим вспомогательные функции"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a7af8900-d5d7-45b3-ab03-ed4e6e75043f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def char2int(char_sorted) -> dict[str,int]:\n",
    "    return {ch:i for i,ch in enumerate(char_sorted)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "db618728-f4cb-4280-a852-41a96b2700f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text2int(text, char_encoded) -> np.ndarray[int]:\n",
    "    return np.array([char_encoded[ch] for ch in text], dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "695a4fde-9a40-4745-abf0-5f3af6cebe31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def int2text(text_encoded, char_array) -> str:\n",
    "    return ''.join(char_array[text_encoded]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5622d36a-f45e-424a-ac71-88cd40394559",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Закодированное предложение: \n",
      "[28 67 61 58  1 68 55 51 55  1 54 50 54 69 68  1 61 58 63 64 52 50 63 63\n",
      " 69 80  1 51 69 62 50 53 69  6  1 65 58 74 58  1 65 64 65 55 66 55 60  8]\n",
      "\n",
      "Декодированное предложение: \n",
      "Если тебе дадут линованную бумагу, пиши поперек.\n"
     ]
    }
   ],
   "source": [
    "char_sorted = sorted(char_set)\n",
    "char_encoded = char2int(char_sorted)\n",
    "char_array = np.array(char_sorted)\n",
    "\n",
    "encoding_test_sample = text2int(text[:48], char_encoded)\n",
    "print(f'Закодированное предложение: \\n{encoding_test_sample}')\n",
    "print()\n",
    "decoding_test_sample = int2text(encoding_test_sample, char_array)\n",
    "print(f'Декодированное предложение: \\n{decoding_test_sample}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ad127a77-8a51-4c41-af78-56d85a95ccb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размерность закодированного текста: (270915,)\n"
     ]
    }
   ],
   "source": [
    "text_encoded = text2int(text, char_encoded)\n",
    "\n",
    "print(f'Размерность закодированного текста: {text_encoded.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef1f601-11ef-4337-8e72-f2a741216264",
   "metadata": {},
   "source": [
    "## **Формирование датасета**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3ce6ab79-5e7b-44c4-95b6-9a9cda1fb617",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28 -> Е\n",
      "67 -> с\n",
      "61 -> л\n",
      "58 -> и\n",
      "1 ->  \n",
      "68 -> т\n",
      "55 -> е\n",
      "51 -> б\n",
      "55 -> е\n",
      "1 ->  \n"
     ]
    }
   ],
   "source": [
    "ds_text_encoded = tf.data.Dataset.from_tensor_slices(text_encoded)\n",
    "\n",
    "for example in ds_text_encoded.take(10):\n",
    "    print(f'{example.numpy()} -> {char_array[example.numpy()]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d9e75f-a0ea-4df3-b98b-1518595a2554",
   "metadata": {},
   "source": [
    "Определм функцию для формирования `x` и `y`. \n",
    "- `x` - уже сгенерированные символы\n",
    "- `y` - следующий необходимый символ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "311a9344-2af5-4c63-95ca-e290e55c2fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_input_target(chunk):\n",
    "    input_seq = chunk[:-1]\n",
    "    target_seq = chunk[1:]\n",
    "    return input_seq, target_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "16d058d5-0def-493b-a099-f44a86b3739f",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 40\n",
    "chunk_size = seq_length + 1\n",
    "\n",
    "ds_chunks = ds_text_encoded.batch(chunk_size, drop_remainder=True)\n",
    "ds_sequences = ds_chunks.map(split_input_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d1d41ba-411f-4f06-8e53-7969355d40fb",
   "metadata": {},
   "source": [
    "## **Разделение данных на мини-пакеты**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "87218546-76c7-4185-8154-e6ba2c9bc20b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Вход Х:\n",
      "[[62  1 67 ... 65 66 64]\n",
      " [68 66 55 ... 61 58  8]\n",
      " [66 64 56 ... 60 63 58]\n",
      " ...\n",
      " [58 72 64 ...  1 51 61]\n",
      " [61 80 54 ... 58 53 58]\n",
      " [ 8  1 27 ... 55 66 68]]\n",
      "Цель Y:\n",
      "[[ 1 67 60 ... 66 64 67]\n",
      " [66 55 52 ... 58  8 20]\n",
      " [64 56 50 ... 63 58 53]\n",
      " ...\n",
      " [72 64 19 ... 51 61 58]\n",
      " [80 54 81 ... 53 58  1]\n",
      " [ 1 27 66 ... 66 68 50]]\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(451)\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 10000\n",
    "ds = ds_sequences.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "\n",
    "for example in ds.take(1):\n",
    "    print(f'Вход Х:\\n{example[0]}')\n",
    "    print(f'Цель Y:\\n{example[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d67a746f-26cf-489a-8e7a-8e1f9e42a3e8",
   "metadata": {},
   "source": [
    "## **Построение модели на основе RNN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "00cedce1-62d7-4e0d-91d1-061165f85ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(vocab_size, embedding_dim, rnn_units):\n",
    "    model = tf.keras.Sequential([\n",
    "        layers.Embedding(vocab_size, embedding_dim),\n",
    "        layers.LSTM(rnn_units, return_sequences=True),\n",
    "        layers.Dense(vocab_size)\n",
    "    ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c83f19-503f-4373-a81b-d3265e6876bd",
   "metadata": {},
   "source": [
    "## **Установка параметров обучения**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3db0ded5-6393-479a-9f87-2988a172340d",
   "metadata": {},
   "outputs": [],
   "source": [
    "charset_size = len(char_array)\n",
    "embedding_dim = 256\n",
    "rnn_units = 512\n",
    "\n",
    "tf.random.set_seed(451)\n",
    "model = build_model(vocab_size=charset_size, \n",
    "                    embedding_dim=embedding_dim,\n",
    "                    rnn_units=rnn_units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "89339030-9d9e-4352-a3b8-c32f20ac751b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, None, 256)         22528     \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, None, 512)         1574912   \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, None, 88)          45144     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,642,584\n",
      "Trainable params: 1,642,584\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed458a49-e073-4971-ba42-0105272abca0",
   "metadata": {},
   "source": [
    "## **Обучение**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0700a470-b954-4384-97ea-54191d8208a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', \n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a3fc5490-5f22-4bd6-8522-07988c7c10c6",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "104/104 [==============================] - 25s 221ms/step - loss: 3.1976\n",
      "Epoch 2/40\n",
      "104/104 [==============================] - 23s 223ms/step - loss: 2.5570\n",
      "Epoch 3/40\n",
      "104/104 [==============================] - 24s 224ms/step - loss: 2.3810\n",
      "Epoch 4/40\n",
      "104/104 [==============================] - 23s 223ms/step - loss: 2.2609\n",
      "Epoch 5/40\n",
      "104/104 [==============================] - 23s 223ms/step - loss: 2.1663\n",
      "Epoch 6/40\n",
      "104/104 [==============================] - 23s 222ms/step - loss: 2.0851\n",
      "Epoch 7/40\n",
      "104/104 [==============================] - 23s 222ms/step - loss: 2.0115\n",
      "Epoch 8/40\n",
      "104/104 [==============================] - 23s 223ms/step - loss: 1.9477\n",
      "Epoch 9/40\n",
      "104/104 [==============================] - 23s 223ms/step - loss: 1.8899\n",
      "Epoch 10/40\n",
      "104/104 [==============================] - 23s 223ms/step - loss: 1.8389\n",
      "Epoch 11/40\n",
      "104/104 [==============================] - 24s 224ms/step - loss: 1.7921\n",
      "Epoch 12/40\n",
      "104/104 [==============================] - 24s 224ms/step - loss: 1.7492\n",
      "Epoch 13/40\n",
      "104/104 [==============================] - 24s 230ms/step - loss: 1.7098\n",
      "Epoch 14/40\n",
      "104/104 [==============================] - 23s 220ms/step - loss: 1.6729\n",
      "Epoch 15/40\n",
      "104/104 [==============================] - 23s 221ms/step - loss: 1.6396\n",
      "Epoch 16/40\n",
      "104/104 [==============================] - 23s 222ms/step - loss: 1.6067\n",
      "Epoch 17/40\n",
      "104/104 [==============================] - 23s 221ms/step - loss: 1.5751\n",
      "Epoch 18/40\n",
      "104/104 [==============================] - 23s 220ms/step - loss: 1.5438\n",
      "Epoch 19/40\n",
      "104/104 [==============================] - 23s 220ms/step - loss: 1.5150\n",
      "Epoch 20/40\n",
      "104/104 [==============================] - 23s 221ms/step - loss: 1.4844\n",
      "Epoch 21/40\n",
      "104/104 [==============================] - 23s 222ms/step - loss: 1.4571\n",
      "Epoch 22/40\n",
      "104/104 [==============================] - 23s 221ms/step - loss: 1.4286\n",
      "Epoch 23/40\n",
      "104/104 [==============================] - 23s 221ms/step - loss: 1.3987\n",
      "Epoch 24/40\n",
      "104/104 [==============================] - 23s 221ms/step - loss: 1.3696\n",
      "Epoch 25/40\n",
      "104/104 [==============================] - 23s 222ms/step - loss: 1.3409\n",
      "Epoch 26/40\n",
      "104/104 [==============================] - 24s 228ms/step - loss: 1.3110\n",
      "Epoch 27/40\n",
      "104/104 [==============================] - 23s 220ms/step - loss: 1.2802\n",
      "Epoch 28/40\n",
      "104/104 [==============================] - 23s 221ms/step - loss: 1.2497\n",
      "Epoch 29/40\n",
      "104/104 [==============================] - 23s 221ms/step - loss: 1.2186\n",
      "Epoch 30/40\n",
      "104/104 [==============================] - 23s 219ms/step - loss: 1.1859\n",
      "Epoch 31/40\n",
      "104/104 [==============================] - 23s 221ms/step - loss: 1.1548\n",
      "Epoch 32/40\n",
      "104/104 [==============================] - 23s 218ms/step - loss: 1.1228\n",
      "Epoch 33/40\n",
      "104/104 [==============================] - 23s 218ms/step - loss: 1.0915\n",
      "Epoch 34/40\n",
      "104/104 [==============================] - 23s 218ms/step - loss: 1.0574\n",
      "Epoch 35/40\n",
      "104/104 [==============================] - 23s 218ms/step - loss: 1.0249\n",
      "Epoch 36/40\n",
      "104/104 [==============================] - 23s 219ms/step - loss: 0.9917\n",
      "Epoch 37/40\n",
      "104/104 [==============================] - 23s 220ms/step - loss: 0.9587\n",
      "Epoch 38/40\n",
      "104/104 [==============================] - 23s 220ms/step - loss: 0.9259\n",
      "Epoch 39/40\n",
      "104/104 [==============================] - 23s 220ms/step - loss: 0.8929\n",
      "Epoch 40/40\n",
      "104/104 [==============================] - 23s 221ms/step - loss: 0.8590\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(ds, epochs=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c8c8ad-6ded-4f97-b7a7-2bd049b922c0",
   "metadata": {},
   "source": [
    "## **Генерация новых отрывков текста**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b38f7ce0-1178-4349-87d9-21b8f5c416a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(model, starting_str, len_generated_text=500, max_input_length=40, scale_factor=1.0):\n",
    "    encoded_input = text2int(starting_str, char_encoded)\n",
    "    encoded_input = tf.reshape(encoded_input, (1,-1))\n",
    "    \n",
    "    generated_str = starting_str\n",
    "    \n",
    "    model.reset_states()\n",
    "    for i in range(len_generated_text):\n",
    "        logits = model(encoded_input)\n",
    "        logits = tf.squeeze(logits, 0)\n",
    "        \n",
    "        scaled_logits = logits * scale_factor\n",
    "        new_char_index = tf.random.categorical(scaled_logits, num_samples=1)\n",
    "        \n",
    "        new_char_index = tf.squeeze(new_char_index)[-1].numpy()\n",
    "        \n",
    "        generated_str += str(char_array[new_char_index])\n",
    "        \n",
    "        new_char_index = tf.expand_dims([new_char_index], 0)\n",
    "        encoded_input = tf.concat([encoded_input, new_char_index], axis=1)\n",
    "        encoded_input = encoded_input[:, -max_input_length:]\n",
    "        \n",
    "    return generated_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d5c10b96-2608-4ca3-a7b1-839a23751644",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Они сжигали книги и испорающие своей мягко самого пять задевать с ней снять.\n",
      "— Я повершен с вами имече, — я говорит: «Мне не было эту ковида. Качертает. Мы словно не «солнее нофью из-за усового обошлом. Воздух огнев земле.\n",
      "— Чудст! Не металла, скряды? — Брандме»се» в один поглядым стоблению.\n",
      "Надвое! Это знает ряде, чтобы и не присятся с глаза от бран двейной тишине, словно на книгу по мерторга:\n",
      "— Монтэг, я почувствиева, началось, вы — это нельзя»! Я и сего на это сделать.\n",
      "— Вы знаете, что, вы не виноватым, потом \n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(451)\n",
    "print(sample(model, 'Они сжигали книги ', scale_factor=1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "3eaf89cf-e2e5-4933-b291-41e5fa807a2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Они сжигали книги и грохотала из грубоко взгляды, на котором поджигалия, как теперь все это заборлить и окончилась война. Она подошел к вам нога. Он поднял с ней под конце конценцими, скрывающихся в темноту, не слова не задумаю свои коснулся его сквозь эту неделю по кругу в ваше двое и смотреть, как он стоял в темноте по домом, как они видел ноги, о вашем мне по улицу волосы, он остановился на полу. Он несколько раз повернул и опустился на стену, где можно было сохнать и все станет свои грубоко засунул руку краск\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(451)\n",
    "print(sample(model, 'Они сжигали книги ', scale_factor=5.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "12b442f7-3ab7-4445-be02-fe07b83601c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Они сжигали книги и их ды ющу ригодцвой корп«седои, — кигивает рубки.\n",
      "— вряд рошки.\n",
      "Заучи те: Что и теать!» — когда»-Громой маюрь, —ы, Фаба видеіь! И говорю, чуть ли 'дрей тогу рядоко, Гослуш.\n",
      "Ее золщное стены, как Клари.\n",
      "Вопья: в резан самом бежня я щип..\n",
      "Онта, ичел. Млежке еслил съе.\n",
      "Стоки! Я птил, чтобы здарыт Тониющую голустурыр фаигнав запом. Ко вбив, лишиний ничнастлизу детел, — отлиюте, Шеспроси, чьи-куру, а?\n",
      "Ктара — только раношэ… Он влоя?\n",
      "— Видите ражьзя какой-то щекс — будто чтояь уностейших, станупый, \n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(451)\n",
    "print(sample(model, 'Они сжигали книги ', scale_factor=0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730cb3d2-e8c2-4707-a0a1-812e4cc2fd18",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
